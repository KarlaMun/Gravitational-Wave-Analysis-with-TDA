{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using giotto-tda for persistent homology\n",
    "\n",
    "First lets write a function to create nice data ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def make_point_clouds(n_samples_per_shape: int, n_points: int, noise: float):\n",
    "    \"\"\"Make point clouds for circles, spheres, and tori with random noise.\n",
    "    \"\"\"\n",
    "    circle_point_clouds = [\n",
    "        np.asarray(\n",
    "            [\n",
    "                [np.sin(t) + noise * (np.random.rand(1)[0] - 0.5), np.cos(t) + noise * (np.random.rand(1)[0] - 0.5), 0]\n",
    "                for t in range((n_points ** 2))\n",
    "            ]\n",
    "        )\n",
    "        for kk in range(n_samples_per_shape)\n",
    "    ]\n",
    "    # label circles with 0\n",
    "    circle_labels = np.zeros(n_samples_per_shape)\n",
    "\n",
    "    sphere_point_clouds = [\n",
    "        np.asarray(\n",
    "            [\n",
    "                [\n",
    "                    np.cos(s) * np.cos(t) + noise * (np.random.rand(1)[0] - 0.5),\n",
    "                    np.cos(s) * np.sin(t) + noise * (np.random.rand(1)[0] - 0.5),\n",
    "                    np.sin(s) + noise * (np.random.rand(1)[0] - 0.5),\n",
    "                ]\n",
    "                for t in range(n_points)\n",
    "                for s in range(n_points)\n",
    "            ]\n",
    "        )\n",
    "        for kk in range(n_samples_per_shape)\n",
    "    ]\n",
    "    # label spheres with 1\n",
    "    sphere_labels = np.ones(n_samples_per_shape)\n",
    "\n",
    "    torus_point_clouds = [\n",
    "        np.asarray(\n",
    "            [\n",
    "                [\n",
    "                    (2 + np.cos(s)) * np.cos(t) + noise * (np.random.rand(1)[0] - 0.5),\n",
    "                    (2 + np.cos(s)) * np.sin(t) + noise * (np.random.rand(1)[0] - 0.5),\n",
    "                    np.sin(s) + noise * (np.random.rand(1)[0] - 0.5),\n",
    "                ]\n",
    "                for t in range(n_points)\n",
    "                for s in range(n_points)\n",
    "            ]\n",
    "        )\n",
    "        for kk in range(n_samples_per_shape)\n",
    "    ]\n",
    "    # label tori with 2\n",
    "    torus_labels = 2 * np.ones(n_samples_per_shape)\n",
    "\n",
    "    point_clouds = np.concatenate((circle_point_clouds, sphere_point_clouds, torus_point_clouds))\n",
    "    labels = np.concatenate((circle_labels, sphere_labels, torus_labels))\n",
    "\n",
    "    return point_clouds, labels\n",
    "\n",
    "\n",
    "# esferas y toros ;)\n",
    "n_samples_per_class = 10\n",
    "point_clouds, labels = make_point_clouds(n_samples_per_class, 10, 0.1)\n",
    "point_clouds.shape\n",
    "print(f\"There are {point_clouds.shape[0]} point clouds in {point_clouds.shape[2]} dimensions, \"\n",
    "      f\"each with {point_clouds.shape[1]} points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate persistent homology\n",
    "\n",
    "``VietorisRipsPersistence``, and all other\n",
    "“persistent homology” transformers in ``gtda.homology``, expect input in\n",
    "the form of a 3D array or, in some cases, a list of 2D arrays. For each\n",
    "entry in the input (here, for each point cloud in ``point_clouds``) they\n",
    "compute a topological summary which is also a 2D array, and then stack\n",
    "all these summaries into a single output 3D array. So, in our case,\n",
    "``diagrams[i]`` represents the topology of ``point_clouds[i]``.\n",
    "``diagrams[i]`` is interpreted as follows: - Each row is a triplet\n",
    "describing a single topological feature found in ``point_clouds[i]``. -\n",
    "The first and second entries (respectively) in the triplet denote the\n",
    "values of the “filtration parameter” at which the feature appears or\n",
    "disappears respectively. They are referred to as the “birth” and “death”\n",
    "values of the feature (respectively). The meaning of “filtration\n",
    "parameter” depends on the specific transformer, but in the case of\n",
    "``VietorisRipsPersistence`` on point clouds it has the interpretation of\n",
    "a length scale. - A topological feature can be a connected component, 1D\n",
    "hole/loop, 2D cavity, or more generally :math:`d`-dimensional “void”\n",
    "which exists in the data at scales between its birth and death values.\n",
    "The integer :math:`d` is the *homology dimension* (or degree) of the\n",
    "feature and is stored as the third entry in the triplet. In this\n",
    "example, the shapes should have 2D cavities so we explicitly tell\n",
    "``VietorisRipsPersistence`` to look for these by using the\n",
    "``homology_dimensions`` parameter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.plotting import plot_diagram\n",
    "\n",
    "VR = VietorisRipsPersistence(homology_dimensions=[0, 1, 2])  # Parameter explained in the text\n",
    "diagrams = VR.fit_transform(point_clouds)\n",
    "# diagrams.shape\n",
    "\n",
    "i = 0\n",
    "plot_diagram(diagrams[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features\n",
    "Instantiate a ``PersistenceEntropy`` transformer and extract scalar features from the persistence diagrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.diagrams import PersistenceEntropy\n",
    "\n",
    "PE = PersistenceEntropy()\n",
    "features = PE.fit_transform(diagrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the new features in a standard classifier\n",
    "Leverage the compatibility with ``scikit-learn`` to perform a train-test split and score the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(features, labels)\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encapsulate the steps above in a pipeline\n",
    "- Define an end-to-end pipeline by chaining transformers from giotto-tda with scikit-learn ones\n",
    "\n",
    "- Train-test split the input point cloud data and labels.\n",
    "\n",
    "- Fit the pipeline on the training data.\n",
    "\n",
    "- Score the fitted pipeline on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "steps = [VietorisRipsPersistence(homology_dimensions=[0, 1, 2]),\n",
    "         PersistenceEntropy(),\n",
    "         RandomForestClassifier()]\n",
    "\n",
    "pipeline = make_pipeline(*steps)\n",
    "\n",
    "pcs_train, pcs_valid, labels_train, labels_valid = train_test_split(point_clouds, labels)\n",
    "pipeline.fit(pcs_train, labels_train)\n",
    "pipeline.score(pcs_valid, labels_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
